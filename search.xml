<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hexo + Next 博客配置记录</title>
    <url>/posts/1962388397/</url>
    <content><![CDATA[<p>很久之前就想写博客，但由于懒加嫌麻烦一直搁置。但逐渐发现学习的东西不断变多，一直记在脑子也难免遗忘，因此还是决定开个博客，一是作为自己平时学习积累的一个记录，梳理平时学习的思路，二来可以将个人浅薄的见解输出，帮助到有需要的人吧。</p>
<p>第一篇博客，当然是从环境配置开始。简单上网搜一下，就知道Hexo + Next + Github + Vscode 应该是简易博客的标配了，Hexo 是基于 Node.js 的静态博客生成器，生成静态页面的速度非常快， Next主题提供多种样式（Muse、Mist、Pisces、Gemini），GitHub Pages 提供免费的静态网站托管服务，Vscode 作为文本编辑器，结合markdown插件，在本地写作时可以Preview博客。对个人博客而言，上述组合应该是绰绰有余啦。</p>
<span id="more"></span>
<h2 id="前置工作"><a href="#前置工作" class="headerlink" title="前置工作"></a>前置工作</h2><h3 id="gitnodejs安装配置"><a href="#Git-Node-js安装配置" class="headerlink" title="Git/Node.js安装配置"></a>Git/Node.js安装配置</h3><p>需要提前安装 Node.js / Git，稍微ChatGPT一下就好啦。Git 需要生成的id_rsa.pub 添加到Github SSH key中。然后测试 SSH连接是否成功<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure></p>
<h3 id="hexo环境安装"><a href="#Hexo环境安装" class="headerlink" title="Hexo环境安装"></a>Hexo环境安装</h3><p>在安装完 Node.js 和 Git 后，可以通过 npm 安装 Hexo 命令行工具。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure><br>此时就可以新建一个blog文件夹了，然后安装相关依赖<br><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="bullet">1.</span> hexo init blog</span><br><span class="line"><span class="bullet">2.</span> 进入上条命令所创建的 blog 文件夹中：cd blog</span><br><span class="line"><span class="bullet">3.</span> 安装相关依赖：npm install</span><br><span class="line"><span class="bullet">4.</span> 启动 Hexo 服务：hexo server(或直接 hexo s)</span><br><span class="line"><span class="bullet">5.</span> npm i hexo-renderer-swig (hexo 在5.0之后把 swig 删除了，需要自己手动安装, 不然渲染出错)</span><br><span class="line"><span class="bullet">6.</span> 访问默认界面，测试是否安装成功：浏览器访问localhost:4000</span><br></pre></td></tr></table></figure></p>
<h3 id="next主题安装"><a href="#Next主题安装" class="headerlink" title="Next主题安装"></a>Next主题安装</h3><p>接下来配置<a href="https://github.com/iissnan/hexo-theme-next">Next</a>主题，这是Hexo的一个主题插件，类似的还有<a href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a>，<a href="https://github.com/ppoffice/hexo-theme-icarus">Icarus</a>，<a href="https://github.com/litten/hexo-theme-yilia">Yilia</a>等等。经过上一步的配置，现在blog文件夹下将会有站点配置文件 _config.yml, 这是Hexo 博客的核心配置文件，用于定义博客的全局设置和功能选项。通过修改 _config.yml，可以自定义博客的外观、功能、部署方式等。</p>
<p>首先进入上一步创建的 blog 文件夹中，将 Next 主题相关文件从 github 克隆到 themes 文件夹中<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/iissnan/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure><br>然后修改 _config.yml中的主题参数<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">theme:</span> <span class="string">next</span></span><br></pre></td></tr></table></figure><br>然后按照Step2 验证一下 localhost:4000的页面渲染效果</p>
<h3 id="github部署"><a href="#Github部署" class="headerlink" title="Github部署"></a>Github部署</h3><p>首先去github 新建一个个人仓库，并设置为公开，仓库名格式为 username.github.io， username就是Github username</p>
<p>在 站点配置文件 _config.yml 找到deploy部分，配置如下<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">&#x27;git&#x27;</span></span><br><span class="line">  <span class="attr">repository:</span> <span class="string">git@github.com:username/username.github.io.git</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">main</span></span><br></pre></td></tr></table></figure><br>在blog项目目录下安装 deployer-git插件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><br>最后生成静态博客并部署到github，过十几分钟就能访问 <a href="https://username.github.io">https://username.github.io</a> 网址看到渲染的页面了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo generate -d</span><br></pre></td></tr></table></figure></p>
<h2 id="next主题优化"><a href="#Next主题优化" class="headerlink" title="Next主题优化"></a>Next主题优化</h2><p>经过上述Next主题安装，在 /themes/next 目录下将会有 主题配置文件 _config.yml。大部分功能都能通过修改config的参数实现和安装相应包实现，具体的一些设置可以参考这篇博文：<a href="https://www.dragonstyle.win/3358042383.html">https://www.dragonstyle.win/3358042383.html</a> ，按照自己的需求去修改。以下是我觉得比较重要，影响写作体验和效果的一些小tips。</p>
<h3 id="公式渲染"><a href="#公式渲染" class="headerlink" title="公式渲染"></a>公式渲染</h3><p>写文章难免会敲latex公式，一般使用MathJax，一个用于在网页中渲染数学公式的JavaScripts库。Hexo默认使用Marked进行渲染，但是却不能渲染mathjax，需要换成 Kramed.<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure><br>然后卸载原来的hexo-math， 安装hexo-renderer-mathjax包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-math --save</span><br><span class="line">npm install hexo-renderer-mathjax --save</span><br></pre></td></tr></table></figure><br>最后修改 主题配置文件中的mathjax 选项，Next版本5.1.4中的配置是这样的<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">mathjax:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">per_page:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">cdn:</span> <span class="string">//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML</span></span><br></pre></td></tr></table></figure><br>最后，为了解决多行公式渲染的问题（<a href="https://github.com/blinkfox/hexo-theme-matery/issues/119">原issue</a>）。在博客根目录下，找到node_modules/kramed/lib/rules/inline.js文件，在inline变量中做出如下修改（以下是修改好后的代码):<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">var inline = &#123;</span><br><span class="line">  // escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, 第 11 行, 将其修改为</span><br><span class="line">  escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,</span><br><span class="line">  autolink: /^&lt;([^ &gt;]+(@|:\/)[^ &gt;]+)&gt;/,</span><br><span class="line">  url: noop,</span><br><span class="line">  html: /^&lt;!--[\s\S]*?--&gt;|^&lt;(\w+(?!:\/|[^\w\s@]*@)\b)*?(?:&quot;[^&quot;]*&quot;|&#x27;[^&#x27;]*&#x27;|[^&#x27;&quot;&gt;])*?&gt;([\s\S]*?)?&lt;\/\1&gt;|^&lt;(\w+(?!:\/|[^\w\s@]*@)\b)(?:&quot;[^&quot;]*&quot;|&#x27;[^&#x27;]*&#x27;|[^&#x27;&quot;&gt;])*?&gt;/,</span><br><span class="line">  link: /^!?\[(inside)\]\(href\)/,</span><br><span class="line">  reflink: /^!?\[(inside)\]\s*\[([^\]]*)\]/,</span><br><span class="line">  nolink: /^!?\[((?:\[[^\]]*\]|[^\[\]])*)\]/,</span><br><span class="line">  reffn: /^!?\[\^(inside)\]/,</span><br><span class="line">  strong: /^__([\s\S]+?)__(?!_)|^\*\*([\s\S]+?)\*\*(?!\*)/,</span><br><span class="line">  // em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 第 20 行，将其修改为 </span><br><span class="line">  em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br><span class="line">  code: /^(`+)\s*([\s\S]*?[^`])\s*\1(?!`)/,</span><br><span class="line">  br: /^ &#123;2,&#125;\n(?!\s*$)/,</span><br><span class="line">  del: noop,</span><br><span class="line">  text: /^[\s\S]+?(?=[\\&lt;!\[_*`$]| &#123;2,&#125;\n|$)/,</span><br><span class="line">  math: /^\$\$\s*([\s\S]*?[^\$])\s*\$\$(?!\$)/,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><br>最后别忘了 hexo clean &amp; hexo g，然后使用 hexo s看效果。</p>
<h3 id="本地markdown-preview图像路径问题"><a href="#本地Markdown-Preview图像路径问题" class="headerlink" title="本地Markdown Preview图像路径问题"></a>本地Markdown Preview图像路径问题</h3><p>使用vscode作为文本编辑器在本地写markdown博客时，需要渲染观察效果，而普通的preview是不会渲染latext公式和图表等信息的，这时候可以换成 Markdown Preview Enhanced 插件。</p>
<p>安装插件后，打开插件的settings， 需要修改默认的 ImageFolder Path 以支持插件找到图像，原本默认的是 /asset（插件安装目录下）， 需要修改到 source/_posts 文件夹， 填写绝对路径即可，比如<br><figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line">D:\blog\source\_posts</span><br></pre></td></tr></table></figure><br>为了便于博客图像管理，可以将站点配置文件中将 <em>post_asset_folder</em> 设置为 true，这样的话当你使用 hexo new title 创建一篇新文章时，Hexo 会自动在 source/_posts 目录下创建一个与文章同名的文件夹，下面可以存放不同博客的图像资源。<br><figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line">source/_posts/</span><br><span class="line">├── <span class="keyword">my</span>-post.md</span><br><span class="line">└── <span class="keyword">my</span>-post/</span><br></pre></td></tr></table></figure><br>而默认情况下，Hexo 的 post_asset_folder 功能只会为每篇文章创建一个资源文件夹，但并不会自动处理图片路径。如果你直接在 Markdown 中引用图片，生成的 HTML 文件中的图片路径可能会出错。这时候需要安装 hexo-asset-image 插件。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-asset-image --save</span><br></pre></td></tr></table></figure><br>然后在写博客时 引用图像就可以使用如下形式, 这样既能 在本地和网页中就都能找到正确的图像路径并且进行渲染了。<br><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">![<span class="string">图片描述</span>](<span class="link">./my-post/image.png</span>)</span><br><span class="line"><span class="emphasis">*<span class="language-xml"><span class="tag">&lt;<span class="name">center</span>&gt;</span></span>图 1: xxx <span class="language-xml"><span class="tag">&lt;/<span class="name">center</span>&gt;</span></span>*</span></span><br></pre></td></tr></table></figure></p>
<h3 id="评论系统"><a href="#评论系统" class="headerlink" title="评论系统"></a>评论系统</h3><p>看网上使用各种的都有，看了一眼主题配置文件和源码， 有这么些是 Next 5.1.4 版本支持的。源码在 /next/layout/_partials/comments.swig 文件中，如下所示<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% if page.comments %&#125;</span><br><span class="line"></span><br><span class="line">  &#123;% if (theme.duoshuo and theme.duoshuo.shortname) or theme.duoshuo_shortname %&#125;</span><br><span class="line">    &lt;div class=&quot;comments&quot; id=&quot;comments&quot;&gt;</span><br><span class="line">      &lt;div class=&quot;ds-thread&quot; data-thread-key=&quot;&#123;&#123; page.path &#125;&#125;&quot;</span><br><span class="line">           data-title=&quot;&#123;&#123; page.title &#125;&#125;&quot; data-url=&quot;&#123;&#123; page.permalink &#125;&#125;&quot;&gt;</span><br><span class="line">      &lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line"></span><br><span class="line">  &#123;% elseif theme.facebook_sdk.enable and theme.facebook_comments_plugin.enable %&#125;</span><br><span class="line">    &lt;div class=&quot;comments&quot; id=&quot;comments&quot;&gt;</span><br><span class="line">      &lt;div class=&quot;fb-comments&quot;</span><br><span class="line">           data-href=&quot;&#123;&#123; page.permalink &#125;&#125;&quot;</span><br><span class="line">           data-numposts=&quot;&#123;&#123; theme.facebook_comments_plugin.num_of_posts &#125;&#125;&quot;</span><br><span class="line">           data-width=&quot;&#123;&#123; theme.facebook_comments_plugin.width &#125;&#125;&quot;</span><br><span class="line">           data-colorscheme=&quot;&#123;&#123; theme.facebook_comments_plugin.scheme &#125;&#125;&quot;&gt;</span><br><span class="line">      &lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">  ......</span><br><span class="line"></span><br><span class="line">  &#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% endif %&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>试了一下推荐比较多的Gitment，发现老是登入出问题，无法评论，而且看网上说需要每次手动初始化评论？Valine 也是类似的需要注册申请，填写 appid 和appkey，就没有再试了，gitalk配置好了又渲染不出来，很奇怪。最后看到了Utterances， 也是类似Gitment使用github issue存储评论，看安装比较简单，于是安装了一下，并且能生效。</p>
<p>首先点击这个网址安装Utterances <a href="https://github.com/apps/utterances">Install Utterances App</a>，一路默认就好。然后修改主题配置，添加 Utterances 相关的配置项，<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">utterances:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">dyyoungg/dyyoungg.github.io</span> <span class="comment"># Github repository owner and name</span></span><br><span class="line">  <span class="comment"># Available values: pathname | url | title | og:title</span></span><br><span class="line">  <span class="attr">issue_term:</span> <span class="string">title</span></span><br><span class="line">  <span class="attr">label:</span> <span class="string">comments</span> </span><br><span class="line">  <span class="comment"># Available values: github-light | github-dark | preferred-color-scheme | github-dark-orange | icy-dark | dark-blue | photon-dark | boxy-light</span></span><br><span class="line">  <span class="attr">theme:</span> <span class="string">github-light</span></span><br></pre></td></tr></table></figure><br>并在 /next/layout/_partials/comments.swig 文件中添加以下代码，就是多补充一个elif逻辑。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% elseif theme.utterances.enable %&#125;</span><br><span class="line">    &lt;div class=&quot;comments&quot; id=&quot;comments&quot;&gt;</span><br><span class="line">      &lt;script src=&quot;https://utteranc.es/client.js&quot;</span><br><span class="line">              repo=&quot;&#123;&#123; theme.utterances.repo &#125;&#125;&quot;</span><br><span class="line">              issue-term=&quot;&#123;&#123; theme.utterances.issue_term &#125;&#125;&quot;</span><br><span class="line">              label=&quot;&#123;&#123; theme.utterances.label &#125;&#125;&quot;</span><br><span class="line">              theme=&quot;&#123;&#123; theme.utterances.theme &#125;&#125;&quot;</span><br><span class="line">              crossorigin=&quot;anonymous&quot;</span><br><span class="line">              async&gt;</span><br><span class="line">      &lt;/script&gt;</span><br><span class="line">    &lt;/div&gt;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo博客搭建</tag>
        <tag>Next主题优化</tag>
      </tags>
  </entry>
  <entry>
    <title>如何优雅的让LLM成为洗数据工具人</title>
    <url>/posts/2274603809/</url>
    <content><![CDATA[<p>相信实际工作中，使用LLM为业务打工应该是大模型时代每个算法工程师都经历过的。作为算法工程师，日常有一部分工作就是洗数据，但看起来简单的需求，实际上也有一些工程问题需要考虑。这篇文章将结合本人日常清洗各种语料的经验，谈一谈如何构建一个高效的处理流程。</p>
<span id="more"></span>
<p><img src="/posts/2274603809/3b7d1bf3b351fddcb9c678ff8b245a80.png" alt="数据清洗流程"></p>
<h2 id="流程要求"><a href="#流程要求" class="headerlink" title="流程要求"></a>流程要求</h2><p>当你有一个可以本地部署的llm，一个高效可靠的处理流程需要考虑哪些问题？我觉得有几点是必须要考虑到的</p>
<ol>
<li>数据规模：当数据规模上去之后，内存的压力变大；因各种原因失败后如何恢复断点处理？</li>
<li>推理效率: 如何压榨LLM的推理性能，提高并发吞吐量</li>
<li>数据质量: LLM 输出的结果需要符合预期格式；对于处理失败的数据，需要自动重试</li>
<li>扩展性与可维护性：模块化架构，便于未来扩展或调整流程。例如，新增数据预处理或后处理模块，可以灵活应对不同类型的数据需求。</li>
</ol>
<h2 id="流程设计"><a href="#流程设计" class="headerlink" title="流程设计"></a>流程设计</h2><p>为了满足上述需求，一个很直观的方案是使用 <a href="https://github.com/vllm-project/vllm">VLLM</a>/<a href="https://github.com/InternLM/lmdeploy">LMdeploy</a>等作为推理框架，并结合异步请求，充分利用IO等待时间，提高系统整体吞吐量。这样的话就能充分利用LLM 的推理能力和异步处理的高并发优势，从而大幅提升数据处理效率。而对于数据侧，可以采用分块和生成器模式，实现数据的流式加载和处理。 并且各个组件之间可以通过队列解耦，降低系统复杂度。</p>
<h3 id="数据流抽象"><a href="#数据流抽象" class="headerlink" title="数据流抽象"></a>数据流抽象</h3><figure class="highlight css"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    <span class="selector-tag">A</span><span class="selector-attr">[Data Source]</span> --&gt; <span class="selector-tag">B</span><span class="selector-attr">[Input Queue]</span></span><br><span class="line">    <span class="selector-tag">B</span> --&gt; C<span class="selector-attr">[Data Generator]</span></span><br><span class="line">    C --&gt; D<span class="selector-attr">[Workers]</span></span><br><span class="line">    D --&gt; E<span class="selector-attr">[Output Queue]</span></span><br><span class="line">    E --&gt; F<span class="selector-attr">[Writer]</span></span><br></pre></td></tr></table></figure>
<h3 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h3><h4 id="1-data-generator"><a href="#1-Data-Generator" class="headerlink" title="1. Data Generator"></a>1. Data Generator</h4><p>为了处理不同场景的数据加载需求，我们需要设计多层次的生成器，一是考虑到可能的大文件切分，因此这部分可以是一个生成器，二是需要考虑每条数据的长度限制，因为需要调用LLM，所以单次长度是有限制的，并且在业务处理过程中，也是需要调整token长度去看效果，因此抽象出一个类，用于数据的生成。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABC, abstractmethod</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>, Generator, <span class="type">Any</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataProcessor</span>(<span class="title class_ inherited__">ABC</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, max_token, tokenizer</span>):</span><br><span class="line">        <span class="variable language_">self</span>.max_length = max_token</span><br><span class="line">        <span class="variable language_">self</span>.tokenizer = tokenizer</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_data</span>(<span class="params">self, file_path: <span class="built_in">str</span></span>) -&gt; Generator[<span class="type">Any</span>, <span class="literal">None</span>, <span class="literal">None</span>]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Load data from file path and yield items one by one</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            file_path: Path to the data file</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">        Yields:</span></span><br><span class="line"><span class="string">            Individual data items from the file</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">preprocess_data</span>(<span class="params">self, data: <span class="type">Any</span></span>) -&gt; <span class="type">Any</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        preprocess each dataitem</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">chunk_data</span>(<span class="params">self, data: <span class="type">Any</span>, chunk_size: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">Any</span>]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        chunk strategy</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="2-llm接口"><a href="#2-LLM接口" class="headerlink" title="2. LLM接口"></a>2. LLM接口</h4><p>LLM的推理肯定是有延迟的，如果使用同步调用，大量时间会浪费在IO等待上，因此通过异步处理，我们可以在等待LLM响应时处理其他请求，显著提高系统吞吐量。假设目前是有显卡资源部署开源模型，我们可以很方便的利用 <a href="https://github.com/vllm-project/vllm">VLLM</a>/<a href="https://github.com/InternLM/lmdeploy">LMdeploy</a> 这样的推理框架写一个LLM 异步请求的抽象类，用于处理LLM的请求调用，结果解析，以及解析失败时的重试机制。</p>
<p>以下以 LMdeploy 为例，代码结构如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> AsyncOpenAI</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LLMInterface</span>(<span class="title class_ inherited__">ABC</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                base_url, </span></span><br><span class="line"><span class="params">                max_retries=<span class="number">3</span>, </span></span><br><span class="line"><span class="params">                retry_delay=<span class="number">1</span>, </span></span><br><span class="line"><span class="params">                max_tokens=<span class="number">8192</span>, </span></span><br><span class="line"><span class="params">                top_p=<span class="number">0.8</span>, </span></span><br><span class="line"><span class="params">                temperature=<span class="number">0.2</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.client = AsyncOpenAI(</span><br><span class="line">            api_key=<span class="string">&#x27;YOUR_API_KEY&#x27;</span>,</span><br><span class="line">            base_url=base_url</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.max_retries = max_retries</span><br><span class="line">        <span class="variable language_">self</span>.retry_delay = retry_delay</span><br><span class="line">        <span class="variable language_">self</span>.max_tokens = max_tokens</span><br><span class="line">        <span class="variable language_">self</span>.top_p = top_p</span><br><span class="line">        <span class="variable language_">self</span>.temperature = temperature</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, messages</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        以 lmdeploy 部署为例</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            model_cards = <span class="keyword">await</span> <span class="variable language_">self</span>.client.models.<span class="built_in">list</span>()._get_page()</span><br><span class="line">            response = <span class="keyword">await</span> <span class="variable language_">self</span>.client.chat.completions.create(</span><br><span class="line">                model=model_cards.data[<span class="number">0</span>].<span class="built_in">id</span>,</span><br><span class="line">                messages=messages,</span><br><span class="line">                temperature=<span class="variable language_">self</span>.temperature,</span><br><span class="line">                top_p=<span class="variable language_">self</span>.top_p,</span><br><span class="line">                max_tokens=<span class="variable language_">self</span>.max_tokens</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">return</span> response.choices[<span class="number">0</span>].message.content</span><br><span class="line">        <span class="keyword">except</span> openai.OpenAIError <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">f&quot;Error: <span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">process_chunk</span>(<span class="params">self, chunk: <span class="type">Any</span>, prompt: <span class="built_in">str</span></span>) -&gt; <span class="type">Any</span>:</span><br><span class="line">        message = <span class="variable language_">self</span>.build_message(chunk, prompt)</span><br><span class="line">        <span class="keyword">while</span> attempt &lt;=<span class="variable language_">self</span>.max_retries:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                response = <span class="keyword">await</span> <span class="variable language_">self</span>.generate(message)      </span><br><span class="line">                parsed_result = <span class="variable language_">self</span>.parse_response(response)</span><br><span class="line">                <span class="keyword">if</span> parsed_result <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  <span class="comment"># 解析成功</span></span><br><span class="line">                    <span class="keyword">return</span> parsed_result</span><br><span class="line">                <span class="keyword">if</span> attempt &lt; <span class="variable language_">self</span>.max_retries - <span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">await</span> asyncio.sleep(<span class="variable language_">self</span>.retry_delay * (attempt + <span class="number">1</span>))</span><br><span class="line">                    attempt += <span class="number">1</span></span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="keyword">await</span> asyncio.sleep(<span class="variable language_">self</span>.retry_delay * (attempt + <span class="number">1</span>))</span><br><span class="line">                attempt += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>  <span class="comment"># 所有重试都失败</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_message</span>(<span class="params">self, chunk: <span class="built_in">str</span>, prompt:<span class="built_in">str</span></span>) -&gt; <span class="type">List</span>[<span class="type">Dict</span>]:</span><br><span class="line">        input_prompt = prompt.<span class="built_in">format</span>(chunk)</span><br><span class="line">        message = [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: input_prompt&#125;]</span><br><span class="line">        <span class="keyword">return</span> message</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_response</span>(<span class="params">self, response: <span class="built_in">str</span></span>) -&gt; <span class="type">Any</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;解析响应，返回None表示解析失败&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h4 id="3worker"><a href="#3-Worker" class="headerlink" title="3.Worker"></a>3.Worker</h4><p>在整个数据处理流程中，Worker 是连接数据生成和 LLM 调用的核心组件。它的主要职责是协调各个组件的工作，确保数据能够高效且可靠地被处理。一个worker主要有以下一些流程</p>
<ol>
<li><p><strong>数据预处理</strong></p>
<ul>
<li>接收输入队列中的数据</li>
<li>使用 DataProcessor 进行数据分块和预处理</li>
<li>确保数据格式符合 LLM 接口的要求</li>
</ul>
</li>
<li><p><strong>LLM 调用</strong></p>
<ul>
<li>构建符合业务需求的 prompt</li>
<li>调用 LLM 接口获取处理结果</li>
<li>解析和验证 LLM 的返回结果</li>
</ul>
</li>
<li><p><strong>结果处理</strong></p>
<ul>
<li>将处理结果写入输出队列</li>
<li>确保处理结果的格式符合预期</li>
<li>处理可能的解析失败情况</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DataWorker</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                data_processor, </span></span><br><span class="line"><span class="params">                llm_interface, </span></span><br><span class="line"><span class="params">                prompt,</span></span><br><span class="line"><span class="params">                args</span>):</span><br><span class="line">        <span class="variable language_">self</span>.save_dir = args.save_dir</span><br><span class="line">        <span class="variable language_">self</span>.chunk_token = args.chunk_token</span><br><span class="line">        <span class="variable language_">self</span>.llm_interface = llm_interface</span><br><span class="line">        <span class="variable language_">self</span>.data_processor = data_processor</span><br><span class="line">        <span class="variable language_">self</span>.prompt = prompt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">process_data</span>(<span class="params">self, file_path, output_queue</span>):</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> <span class="variable language_">self</span>.data_processor.load_data(file_path):</span><br><span class="line">            all_answers = []</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">for</span> chunk <span class="keyword">in</span> <span class="variable language_">self</span>.data_processor.chunk_data(item, <span class="variable language_">self</span>.chunk_token):</span><br><span class="line">                    task = asyncio.create_task(<span class="variable language_">self</span>.fetch_answer(chunk))</span><br><span class="line">                    result = <span class="keyword">await</span> task</span><br><span class="line">                    <span class="keyword">if</span> result <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        all_answers.append(result)</span><br><span class="line">                dump_data = &#123;<span class="string">&quot;content&quot;</span>: all_answers&#125;</span><br><span class="line">                save_path = os.path.join(<span class="variable language_">self</span>.save_dir, data_id+<span class="string">&quot;.json&quot;</span>)</span><br><span class="line">                <span class="keyword">await</span> output_queue.put((dump_data, save_path))</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">fetch_answer</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">await</span> <span class="variable language_">self</span>.llm_interface.process_chunk(text, <span class="variable language_">self</span>.prompt)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self, queue, output_queue</span>):</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                file_path = <span class="keyword">await</span> queue.get()</span><br><span class="line">                <span class="keyword">if</span> file_path <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    queue.task_done()</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">await</span> <span class="variable language_">self</span>.process_data(file_path, output_queue)</span><br><span class="line">                queue.task_done()</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Worker error: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h3><p>DataPipeline 是整个数据处理流程的 orchestrator，负责协调各个组件的工作，实现数据的流式处理。它将数据加载、预处理、LLM 调用、结果写入和进度显示等步骤整合在一起，形成一个完整的数据处理管道，主要使用两个异步队列管理输入和输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DataPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">    num_processes: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">    llm_base_url: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">    data_processor_factory: <span class="type">Callable</span>, </span></span><br><span class="line"><span class="params">    llm_interface_factory: <span class="type">Callable</span>,</span></span><br><span class="line"><span class="params">    prompt: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">    args</span>):</span><br><span class="line">        <span class="variable language_">self</span>.num_processes = num_processes</span><br><span class="line">        <span class="variable language_">self</span>.save_dir = args.save_dir</span><br><span class="line">        <span class="variable language_">self</span>.chunk_token = args.chunk_token</span><br><span class="line">        <span class="variable language_">self</span>.llm_base_url = llm_base_url</span><br><span class="line">        <span class="variable language_">self</span>.data_processor_factory = data_processor_factory</span><br><span class="line">        <span class="variable language_">self</span>.llm_interface_factory = llm_interface_factory</span><br><span class="line">        <span class="variable language_">self</span>.prompt = prompt</span><br><span class="line">        <span class="variable language_">self</span>.args = args</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.workers = []</span><br><span class="line">        <span class="variable language_">self</span>.start_time = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">_worker</span>(<span class="params">self</span>):</span><br><span class="line">        llm_interface = <span class="variable language_">self</span>.llm_interface_factory(<span class="variable language_">self</span>.llm_base_url)</span><br><span class="line">        data_processor = <span class="variable language_">self</span>.data_processor_factory(max_token=<span class="variable language_">self</span>.chunk_token)</span><br><span class="line">        data_worker = DataWorker(data_processor, llm_interface, <span class="variable language_">self</span>.prompt, <span class="variable language_">self</span>.args)</span><br><span class="line">        <span class="keyword">await</span> data_worker.run(<span class="variable language_">self</span>.queue, <span class="variable language_">self</span>.output_queue)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">_json_writer</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            item = <span class="keyword">await</span> <span class="variable language_">self</span>.output_queue.get()</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            data_dict, save_path = item</span><br><span class="line">            <span class="comment"># write logic</span></span><br><span class="line">            <span class="variable language_">self</span>.output_queue.task_done()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">_display_progress</span>(<span class="params">self, total_files: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            remaining_files = <span class="variable language_">self</span>.queue.qsize()</span><br><span class="line">            processed_files = total_files - remaining_files</span><br><span class="line">            elapsed_time = time.time() - <span class="variable language_">self</span>.start_time</span><br><span class="line">            formatted_time = <span class="built_in">str</span>(timedelta(seconds=<span class="built_in">int</span>(elapsed_time)))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;\rProcessed <span class="subst">&#123;processed_files&#125;</span>/<span class="subst">&#123;total_files&#125;</span> files. Elapsed time: <span class="subst">&#123;formatted_time&#125;</span>&quot;</span>, end=<span class="string">&#x27;&#x27;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">await</span> asyncio.sleep(<span class="number">5</span>)</span><br><span class="line">            <span class="keyword">if</span> remaining_files == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self, data_list: <span class="type">List</span>[<span class="built_in">str</span>]</span>):</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.queue = asyncio.Queue()</span><br><span class="line">        <span class="variable language_">self</span>.output_queue = asyncio.Queue()</span><br><span class="line"></span><br><span class="line">        os.makedirs(<span class="variable language_">self</span>.save_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">for</span> file_path <span class="keyword">in</span> data_list:</span><br><span class="line">            <span class="variable language_">self</span>.queue.put_nowait(file_path)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.start_time = time.time()</span><br><span class="line">        progress_task = asyncio.create_task(<span class="variable language_">self</span>._display_progress(<span class="built_in">len</span>(data_list)))</span><br><span class="line">        writer_task = asyncio.create_task(<span class="variable language_">self</span>._json_writer())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_processes):</span><br><span class="line">            worker_task = asyncio.create_task(<span class="variable language_">self</span>._worker())</span><br><span class="line">            <span class="variable language_">self</span>.workers.append(worker_task)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">await</span> <span class="variable language_">self</span>.queue.join()</span><br><span class="line">        <span class="keyword">await</span> <span class="variable language_">self</span>.output_queue.join()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> worker_task <span class="keyword">in</span> <span class="variable language_">self</span>.workers:</span><br><span class="line">            worker_task.cancel()</span><br><span class="line">        progress_task.cancel()</span><br><span class="line">        writer_task.cancel()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h3><p>有了以上组件，就可以定义主函数<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    主函数，负责解析命令行参数、初始化配置，并启动数据处理流程。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        args: 命令行参数对象。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    流程:</span></span><br><span class="line"><span class="string">        1. 解析命令行参数。</span></span><br><span class="line"><span class="string">        2. 初始化配置，包括保存目录、LLM服务基地址、并发进程数等。</span></span><br><span class="line"><span class="string">        3. 获取数据列表，并根据分片索引和分片数量进行数据分片。</span></span><br><span class="line"><span class="string">        4. 创建 DataPipeline 实例，并传入必要的参数和工厂方法。</span></span><br><span class="line"><span class="string">        5. 启动数据处理流程。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    save_dir = args.save_dir</span><br><span class="line">    os.makedirs(save_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    base_url = <span class="string">f&quot;http://0.0.0.0:<span class="subst">&#123;args.server_port&#125;</span>/v1&quot;</span></span><br><span class="line">    num_processes = args.num_process</span><br><span class="line"></span><br><span class="line">    data_root = args.data_root</span><br><span class="line">    files = get_data_list(data_root, save_dir) <span class="comment"># 需要实现去重，files排列顺序唯一化</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据分片，多节点</span></span><br><span class="line">    chunk_index = args.chunk_index</span><br><span class="line">    chunk_num = args.chunks_num</span><br><span class="line">    chunk_start = <span class="built_in">max</span>(<span class="number">0</span>, <span class="built_in">int</span>(chunk_index / chunk_num * <span class="built_in">len</span>(files)))</span><br><span class="line">    chunk_end = <span class="built_in">min</span>(<span class="built_in">len</span>(files), <span class="built_in">int</span>((chunk_index + <span class="number">1</span>) / chunk_num * <span class="built_in">len</span>(files)))</span><br><span class="line">    data_list = files[chunk_start:chunk_end]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建 DataPipeline 实例</span></span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;&quot;</span>)</span><br><span class="line">    llm_interface_factory = <span class="keyword">lambda</span> base_url: LLMInterface(base_url=base_url)</span><br><span class="line">    data_processor_factory = <span class="keyword">lambda</span> max_token: DataProcessor(max_token=max_token, tokenizer=tokenizer)</span><br><span class="line"></span><br><span class="line">    prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    你需要对以下内容进行修改：</span></span><br><span class="line"><span class="string">    &#123;&#125;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    pipeline = DataPipeline(</span><br><span class="line">        num_processes=num_processes,</span><br><span class="line">        llm_base_url=base_url,</span><br><span class="line">        data_processor_factory=data_processor_factory,</span><br><span class="line">        llm_interface_factory=llm_interface_factory,</span><br><span class="line">        prompt=prompt,</span><br><span class="line">        args=args</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 启动数据处理流程</span></span><br><span class="line">    asyncio.run(pipeline.run(data_list))</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文介绍了一种基于异步处理和流式数据处理的 LLM 数据清洗方案。该方案的核心思想是将数据处理流程分解为多个独立的组件，并通过队列进行解耦，从而实现高效、可靠的数据清洗。</p>
<p>该方案主要包含以下几个核心组件：</p>
<ol>
<li><strong>DataProcessor</strong>: 负责数据的加载、预处理和分块，支持不同类型的数据源。</li>
<li><strong>LLMInterface</strong>: 负责与 LLM 进行交互，处理请求和响应，并实现重试机制。</li>
<li><strong>DataWorker</strong>: 负责协调数据处理流程，包括数据预处理、LLM 调用和结果处理。</li>
<li><strong>DataPipeline</strong>: 负责整个数据处理流程的编排，包括任务分发、结果写入和进度显示。</li>
</ol>
<p>这些组件通过异步队列进行通信，实现数据的流式处理和并发执行。</p>
]]></content>
      <categories>
        <category>数据处理</category>
      </categories>
      <tags>
        <tag>LLM推理框架</tag>
        <tag>asyncio异步</tag>
      </tags>
  </entry>
</search>
